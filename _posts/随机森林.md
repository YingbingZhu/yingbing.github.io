随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树。

每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种Bagging 思想。

* 极好的准确率
* 大数据集
* 处理具有高维特征的输入样本，而且不需要降维
* 能够评估各个特征在分类问题上的重要性
* 在生成过程中，能够获取到内部生成误差的一种无偏差估计/It generates an internal unbiased estimate of the generalization error as the forest building progresses；
* 对于缺失值问题也能够获得很好得结果

**随机森林分类效果（错误率）与两个因素有关：**

* 森林中任意两棵树的相关性：相关性越大，错误率越大；
* 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

 减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。

上面我们提到，构建随机森林的关键问题就是如何选择最优的特征个数，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。

 随机森林有一个重要的优点就是，没有必要对它进行交叉验证或者用一个独立的测试集来获得误差的一个无偏估计。它可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。

 我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。

 而这样的采样特点就允许我们进行oob估计，它的计算方式如下：

 **（note：以样本为单位）**

 1）对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）；

 2）然后以简单多数投票作为该样本的分类结果；

 3）最后用误分个数占样本总数的比率作为随机森林的oob误分率。

 （文献原文：Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.）

oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。

### 

随机森林改变了决策树容易过拟合的问题：
-------------------

1）Boostrap从袋内有放回的抽取样本值

2）每次随机抽取一定数量的特征（通常为sqr(n)）。

 分类问题：采用Bagging投票的方式选择类别频次最高的

 回归问题：直接取每颗树结果的平均值。

