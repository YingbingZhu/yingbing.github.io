# Decision Tree

决策树就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如 CART 只能是二叉树），也可以是多叉树（比如 ID3、C4.5 可以是多叉树或二叉树）。
决策树学习的算法通常是递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程，这一过程对应着特征空间的划分，也对应着决策树的构建。

决策树的三个步骤：特征选择、决策树的生成、决策树的修剪。

分而治之策略
从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点，此时，每个子结点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直到达到叶结点，最后将实例分到叶结点的类中。
递归返回的条件：
1. 当前结点包含的样本全属于同一类别
2. 当前属性集为空，或是所有样本在所有属性上取值相同
3. 当前结点包含样本集合为空
根节点包含整个样本集，每个叶结点都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。

决策树学习的本质：从训练集中归纳出一组分类规则，或者说是由训练数据集估计条件概率模型。
决策树学习的损失函数：正则化的极大似然函数

特征选择准则：
1、信息
信息是用来消除随机不确定性的东西如果带分类的事物集合可以划分为多个类别当中，则某个类（xi）的信息定义如下:   

2、熵
内部的混乱程度
随着树深度的增加，节点的熵迅速下降
熵是对随机变量不确定性的度量，熵便是信息的期望值，可以记作：

熵只依赖X的分布，和X的取值没有关系，熵是用来度量不确定性，当熵越大即这个类别的不确定性更大，反之越小，当随机变量的取值为两个时，熵随概率的变化曲线如下图：

当p=0或p=1时，H(p)=0,随机变量完全没有不确定性，当p=0.5时，H(p)=1,此时随机变量的不确定性最大
条件熵
概率定义：随机变量X在给定条件下随机变量Y的条件熵，对定义描述为：X给定条件下Y的条件概率分布的熵对X的数学期望，在机器学习中为选定某个特征后的熵，公式如下：


3、信息增益
信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，在概率中定义为：待分类的集合的熵和选定某个特征的条件熵之差（这里只的是经验熵或经验条件熵，由于真正的熵并不知道，是根据样本计算出来的），公式如下：



在划分数据集之前之后信息发生的变化（也就是熵的变化）称为信息增益，分别计算每个特征值划分数据集获得的信息增益，选择信息增益最高的特征作为划分特征。
信息增益的计算：



