1\. 概念
======

线性回归（Linear Regression）是一种通过属性的线性组合来进行预测的线性模型，其目的是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。

![](机器学习/resources/DE020B26AD4F5AD7E9C7E83FFA508FA3.png)

2\. 特点
======

1. 优点：结果具有很好的可解释性（w直观表达了各属性在预测中的重要性），计算熵不复杂。
2. 缺点：对非线性数据拟合不好
3. 适用数据类型：数值型和标称型数据

3\. 原理与推导
=========

1\. 给定数据集![D = \\left\\{ {\\left( {{x\_i},{y\_i}} \\right)} \\right\\}\_{i = 1}^m](机器学习/resources/AFC841685C06DCAD2D83736A64A01F59.gif)，其中![x\_i=\\left( {{x\_{i1}},{x\_{i2}}, \\ldots ,{x\_{id}}} \\right)](机器学习/resources/28FE7B773ECB28D7D3A6BC211608E5E4.gif)，![{y\_i} \\in R](机器学习/resources/1579A32CF7B0C342C3975319DA38A54C.gif)（线性回归的输出空间是整个实数空间）。![m](机器学习/resources/E77E1905584D83CF5A3DC7079BEF474F.gif)是样本数，![d](机器学习/resources/A2B14B4AD9EF9A2247F485E0A2B04FD4.gif)是属性维度。

线性回归试图学得：

 ![f\\left( {{x\_i}} \\right) = {w^T}{x\_i} + b](机器学习/resources/A2B727CE202BAC229855CED4B702A2D5.gif) （1），使得![f\\left( {{x\_i}} \\right) \\simeq {y\_i}](机器学习/resources/D821585ABBDC4801A3A5A181F901E747.gif)。

为便于讨论，使![b = {w\_0} \\cdot x{}\_0](机器学习/resources/61B2B713A41AD22B5F0BB2E34BF51EA4.gif)，其中![{x\_0} = 1](机器学习/resources/2ACE80AAC57CD2C0877E83C1EF61F3B0.gif)。此时，![w](机器学习/resources/B003CA8502453B29E8096B2A52C3ABDC.gif)就成为了![w = \\left( {{w\_0},{w\_1}, \\ldots ,{w\_d}} \\right)](机器学习/resources/266BAB145ABE8E0ACC1DCA9D8A994B3A.gif)，![x](机器学习/resources/7790DD0EFB4A03A4C876741804D9B559.gif)就成为了![x\_i=\\left( {{1},{x\_{i1}}, \\ldots ,{x\_{id}}} \\right)](机器学习/resources/625DADA7AC01CE843CA531A1A20B0DE3.gif)，期望学得的函数为![f\\left( {{x\_i}} \\right) = {w^T}{x\_i}](机器学习/resources/BD25B5823F680CE5A28D90BE8A8A62A9.gif)。

2\. 预测值和真实值之间都肯定存在差异![\\varepsilon](机器学习/resources/1002A334546BF7A26F7C90A5DD31C8A5.gif)，对于每个样本：

 ![{y\_i} = {w^T}{x\_i} + {\\varepsilon \_i}](机器学习/resources/33DBDDF64592BC716A355C5452F8761A.gif) （2）

假设误差![{\\varepsilon \_i}](机器学习/resources/D4F5E8C92D78DDDC75194696D2687989.gif)是独立同分布的，并且服从高斯分布。即：

 ![p\\left( {{\\varepsilon \_i}} \\right) = \\frac{1}{{\\sqrt {2\\pi } \\sigma }}\\exp \\left( { - \\frac{{{\\varepsilon \_i}^2}}{{2{\\sigma ^2}}}} \\right)](机器学习/resources/AF1EC6451E45F52F61FD1476E458B297.gif) （3）

将（2）代入（3）中，得到在已知参数![w](机器学习/resources/B003CA8502453B29E8096B2A52C3ABDC.gif)和数据![w\_i](机器学习/resources/494F262C0DED99D8DDD368CEE8FF26D6.gif)的情况下，预测值为![y\_i](机器学习/resources/FE4A361267A36A5EC0842B8C8FBD4253.gif)的条件概率：

 ![p\\left( {{y\_i}\\left| {{x\_i};w} \\right.} \\right) = \\frac{1}{{\\sqrt {2\\pi } \\sigma }}\\exp \\left( { - \\frac{{{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right)](机器学习/resources/EEE85B9BCE3306D0F51A7A74DE083678.gif) （4）

3\. 将（4）连乘得到在已知参数和数据的情况下，预测值为的条件概率，这个条件概率在数值上等于，likelihood（w|x,y），也就是在已知现有数据的条件下，w是真正参数的概率，即似然函数（5）：

 ![L\\left( w \\right) {\\rm{ = }}\\prod\\limits\_{i = 1}^m {p\\left( {{y\_i}\\left| {{x\_i};w} \\right.} \\right)} = \\prod\\limits\_{i = 1}^m {\\frac{1}{{\\sqrt {2\\pi } \\sigma }}} \\exp \\left( { - \\frac{{{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right)](机器学习/resources/32E1A0585905593002EA08A5CC936553.gif) （5）

为什么要引入似然函数：为了根据样本估计参数值。极大化似然函数

为什么要对似然函数进行log变换：由于乘法难解，通过对数可以将乘法转换为加法，简化计算。

对数似然函数：

 ![\\begin{array}{l} \\ell\\left( w \\right) = \\log \\prod\\limits\_{i = 1}^m {\\frac{1}{{\\sqrt {2\\pi } \\sigma }}} \\exp \\left( { - \\frac{{{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right)\\\\ = \\sum\\limits\_{i = 1}^m {\\log \\frac{1}{{\\sqrt {2\\pi } \\sigma }}} \\exp \\left( { - \\frac{{{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right)\\\\ = \\sum\\limits\_{i = 1}^m {\\log \\frac{1}{{\\sqrt {2\\pi } \\sigma }}} + \\sum\\limits\_{i = 1}^m {log\\left( {\\exp \\left( { - \\frac{{{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\right)} \\right)} \\\\ = m\\log \\frac{1}{{\\sqrt {2\\pi } \\sigma }} - \\sum\\limits\_{i = 1}^m {\\frac{{{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}}}{{2{\\sigma ^2}}}} \\\\ = m\\log \\frac{1}{{\\sqrt {2\\pi } \\sigma }} - \\frac{1}{{{\\sigma ^2}}}\\frac{1}{2}\\sum\\limits\_{i = 1}^m {{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}} \\end{array}](机器学习/resources/50A11DC69623413ABC36C46D72E4787C.gif) （6）

得到目标函数：

 ![J(w) = \\frac{1}{2}\\sum\\limits\_{i = 1}^m {{{\\left( {{y\_i} - {w^T}{x\_i}} \\right)}^2}} \\\\ = \\frac { 1 } { 2 } \\left\\| \\left[ \\begin{array} { c } { y \_ { 1 } - w ^ { T } x \_ { 1 } } \\\\ { y \_ { 2 } - w ^ { T } x \_ { 2 } } \\\\ { \\cdots } \\\\ { y \_ { m } - w ^ { T } x \_ { m } } \\end{array} \\right] \\right\\| ^ { 2 }= \\frac { 1 } { 2 } \\left\\| \\left[ \\begin{array} { l } { y \_ { 1 } } \\\\ { y \_ { 2 } } \\\\ { \\cdots } \\\\ { y \_ { m } } \\end{array} \\right] - w ^ { T } \\left[ \\begin{array} { c } { x \_ { 1 } } \\\\ { x \_ { 2 } } \\\\ { \\cdots } \\\\ { x \_ { m } } \\end{array} \\right] \\right\\| ^ { 2 } \\\\ = \\frac{1}{2}{\\left\\| {y - {w^T}X} \\right\\|^2} = \\frac{1}{2}{\\left( {y - {w^T}x} \\right)^T}\\left( {y - {w^T}x} \\right)](机器学习/resources/CA3EC9356B3F5A3B1847E2BE04E2C6AD.gif) （7）（最小二乘法）

 使所有点到曲线的方差最小.利用最小二乘对扫描线上的所有数据点进行拟合,得到一条样条曲线,然后逐点计算每一个点Pi到样条曲线的欧拉距离ei（即点到曲线的最短距离）,ε是距离的阈值，事先给定，如果ei≥ε，则将该点判断为噪点.

为什么要让目标函数越小越好：似然函数表示样本成为真实的概率，似然函数越大越好，也就是目标函数![J\\left( w \\right)](机器学习/resources/3A4E5734B4FAB5DD83189AB3F0AED6F0.gif)越小越好。

4\. 目标函数是凸函数，只要找到一阶导数为0的位置，就找到了最优解。

因此求偏导：

 ![\\begin{array}{l} \\frac{{\\partial J\\left( w \\right)}}{{\\partial w}} = \\frac{1}{2}\\frac{\\partial }{{\\partial w}}\\left( {{{\\left( {y - {w^T}x} \\right)}^T}\\left( {y - {w^T}x} \\right)} \\right)\\\\ = \\frac{1}{2}\\frac{\\partial }{{\\partial w}}\\left( {{{\\left( {y - Xw} \\right)}^T}\\left( {y - Xw} \\right)} \\right)\\\\ = \\frac{1}{2}\\frac{\\partial }{{\\partial w}}\\left( {{w^T}{X^T}Xw - 2{w^T}Xy + {y^T}y} \\right)\\\\ {\\rm{ = }}\\frac{1}{2}\\left( {{X^T}Xw{\\rm{ + }}{X^T}Xw{\\rm{ - }}2Xy} \\right)\\\\ {\\rm{ = }}{X^T}Xw{\\rm{ - }}Xy \\end{array}](机器学习/resources/EE2C3DFC779640E6978B86C521D3AFC3.gif) （8）

5\. 令偏导等于0：

 ![\\frac{{\\partial J\\left( w \\right)}}{{\\partial w}} = {\\rm{0}}](机器学习/resources/96A1B2FB59357DAD953B0AFF55D9893A.gif) （9）

得到：

 ![{X^T}Xw = Xy](机器学习/resources/1719EA5B1D7E7DBBD05193E2037C666E.gif) （10）

情况一：![{X^T}X](机器学习/resources/682360E25884D7558B47FF8ABFB9D7DD.gif)可逆，唯一解。令公式（10）为零可得最优解为：

 ![w^\* = {\\left( {{X^T}X} \\right)^{ - 1}}X^Ty](机器学习/resources/99F5F2628D4EB3A8E1EC75FA33A6F0C2.gif) （11）

 学得的线性回归模型为:

 ![\\mathop y\\limits^ \\wedge = {w^T}X = {X^T}w = {X^T}{\\left( {{X^T}X} \\right)^{ - 1}}{X^T}y](机器学习/resources/3FB9524A5E45619FAD10B03418437B86.gif) （12）

情况二：![{X^T}X](机器学习/resources/682360E25884D7558B47FF8ABFB9D7DD.gif)不可逆，可能有多个解。选择哪一个解作为输出，将有学习算法的偏好决定，常见的做法是增加![\\lambda](机器学习/resources/99D394E7D0B74248114405067E0FFD51.gif)扰动。

 ![{w^\*} = {\\left( {{X^T}X + \\lambda I} \\right)^{ - 1}}{X^T}y](机器学习/resources/058C60AFE807F70826D1AD6056AFB62B.gif) （13）

4\. 算法描述
--------

1\. 从数据集D出发，构建输入矩阵X和输出向量y。

 ![X = \\left[ \\begin{array} { c } { x \_ { 1 } ^ { T } } \\\\ { x \_ { 2 } ^ { T } } \\\\ { \\cdots } \\\\ { x \_ { m } ^ { T } } \\end{array} \\right] \\quad y = \\left[ \\begin{array} { c } { y \_ { 1 } } \\\\ { y \_ { 2 } } \\\\ { \\dots } \\\\ { y \_ { m } } \\end{array} \\right]](机器学习/resources/1A893A3B4CBDB2124F61B9C286026B33.gif) 

2\. 计算伪逆（pseudo-inverse）![{X^ + }](机器学习/resources/F35DFF548538423C589DEB80DBA7790A.gif)。

3. 返回![{w^\*} = {X^ + }y](机器学习/resources/D5B6B677C4B4F0BB776EFF96FF1D47C2.gif)，学得的线性回归模型为![\\mathop y\\limits^ \\wedge = {w^T}X](机器学习/resources/98966BC776E4A942D3B6D49EE7BBCDAA.gif)。

5\. 广义线性回归
----------

当![y](机器学习/resources/65ACD5CAB815E1B2A6EF422F3400C4BF.gif)不再只是线性回归中用到的正态分布，而是扩大为指数族中的任一分布。这样得到的模型称为“广义线性模型”（generalized linear model）：

 ![y = {g^{ - 1}}\\left( {{w^T}x + b} \\right)](机器学习/resources/536BBB8C608A38C899076A2A1A319E5D.gif)

其中函数![g\\left( \\cdot \\right)](机器学习/resources/6CC8448FF0056F70CB982A2D7CE4541D.gif)称为“联系函数”（link function）。