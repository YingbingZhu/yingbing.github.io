Adaboost只适用于二分类问题

GBDT？

把决策回归树作为基学习器，首先初始化使得最小化损失函数，利用前向分步算法，每次迭代时用一个基学习器去拟合整个函数最快下降方向，再求最小化当前损失的线性搜索步长，得到的基学习器和步长放入加性模型，经过M次迭代最后求和，每次迭代还增加了一个收缩系数作为正则化参数，和迭代次数一起调节最终模型的性能。

适用范围：适用于回归问题（线性和非线性），可用于二分类和多分类问题

XGBOOST vs. GBDT?

1\. XGBOOST将模型的复杂度加入正则项（树的叶子节点数量和叶子节点的值有关），避免过拟合，泛化性能优于GBDT

2\. 损失函数用泰勒展开式，加快优化性能，同时用到一阶和二阶导

3\. GBDT只支持CART，XGBOOST还支持线性分类器

4\. 引入特征子采样，降低过拟合，减少计算

5\. 寻找最佳分割点时，实心近似贪心算法（根据百分位法列举几个可能成为分隔点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳），加速和减小内存消耗，可以处理缺失值和稀疏数据集（为缺失值或者指定的值指定分支的默认方向）

6\. 支持并行处理，特征的并行（将特征排序后以block的形式存储在内存中，在后面迭代中重复使用，各个特征的增益计算也可以并行）

GBDT vs. 随机森林？

相同：

都是由多颗树组成，最终结果由多颗树一起决定

不同点：

1\. 组成随机森林的树可以是分类树，也可以是回归树，GBDT只能由回归树组成

2\. 组成随机森林的树可以并行生成，但是组成GBDT的树只能串行

3\. 随机森林采取多数投票，GBDT是将所有结果累加或者加权

4\. 随机森林对异常值不敏感，GBDT对异常值敏感

5\. 随机森林通过减小方差来提高性能，GBDT通过减小偏差来提高性能

随机森林优缺点：

优点：1\. 训练速度块，预测准确度高。 2\. 能够处理高维特征，不用特征选择

3\. 容易进行并行化处理

缺点：在噪声较大的分类或者回归问题上容易过拟合，不适合小样本，适合大样本

GBDT优缺点：

优点：1\. 灵活处理各种类型的数据。2\. 在相对较少的调参时间下，预测准确度较高

缺点：难以并行处理数据

SGBT?

随机梯度提升树，在GBDT的基础上加入随机子采样，不把全部数据做boosting，随机选取一部分，减少计算量，加快学习速度

SVM?

1\. 目标是对特征空间划分的最优超平面，最大化分类边际。

2\. 计算复杂度取决于支持向量的数目，而不是样本空间的维度，避免了维度灾难。3\. 鲁棒性（增删非支持向量对模型没有影响；支持向量具有一定的鲁棒性；SVM对核的选取不敏感）

4\. SVM学习问题可以表示为凸优化问题，可以发现目标函数的全局最小值

5\. SVM在小样本训练集上可以获得好结果。

6\. 基于结构风险最小化原则，避免过拟合问题，泛化能力强

7\. 泛化错误率低，分类速度快，结果易解释。

8\. 主要应用在文本识别，中文分类，人脸识别等。

缺点：

1\. 对大规模样本难以实施，需要存储训练样本和矩阵，求解二次规划来求支持向量，设计m阶矩阵的计算（m个样本）

2\. 用SVM解决多分类问题困难

3\. 对缺失数据敏感，对参数和核函数的选择敏感

SVM的惩罚因子和松弛变量

惩罚因子C决定有多重视离群点带来的损失，C为无限大时，问题退化成硬间隔问题。解决过拟合的方法是引入松弛变量，因为松弛变量能够容忍异常点的存在。

SVM对偶问题？

对偶问题更容易求解，自然引入核函数，进而推广到非线性问题。

LR vs. SVM

相同：都是二分类，都可以增加正则化项

不同：

1\. LR是参数模型，SVM是非参数模型

2\. 损失函数采用的不同，LR是logistical loss, SVM是hinge loss

3\. SVM的处理方法是只考虑支持向量，和分类最相关的少数点，去学习分类器。LR是通过非线性映射，大大减小离分类平面较远的点的权重，相对提升最相关的点的权重。

4\. 逻辑回归模型简单，易理解，对大规模线性分类时方便。SVM复杂，

逻辑回归？

优点：1\. 简单，广泛应用于工业问题

2\. 分类时计算量小，速度快，存储资源低

3\. 便利的观测样本概率分数

4\. 多重共线性不是问题，可以结合L2正则化来解决问题

缺点：

1\. 特征空间大时，逻辑回归性能不好

2\. 容易欠拟合，准确度不高

3\. 不能很好的处理大量多类特征或变量

4\. 只能处理二分类问题（softmax可以处理多分类）

决策树？

优点：

计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征；

缺点：

单颗决策树分类能力弱，并且对连续值变量难以处理；

容易过拟合；

过拟合？

数据：数据不规范，数据量少，统计特征用到了数据标签

算法：算法模型过于复杂

解决：

1\. 数据规范化，处理缺失值，增加数据量，采样，添加噪声数据

2\. 正则化，控制模型复杂度

3\. early stoping, 减少迭代次数，减少树的深度

4\. 调整学习率

5\. 融合几个模型

6\. dropout, regularization, batch normalization

欠拟合？

模型复杂度过低，特征量过少

解决：增加新特征；增加模型复杂度（添加多项式特征）；减少正则化参数

L1和L2

L1：LASSO 向量中每个元素绝对值的和，L1范数的解是稀疏性的。w可取的值是转置的方形，L2 对应的是圆形

L2：岭回归 欧氏距离（平方和的平方根），L2范数越小，可以使得w的每个元素都很小，但是不会等于0

优化算法？

1\. 随机梯度下降：一定程度解决局部最优解的问题；收敛速度较慢

2\. 批量梯度下降：容易陷入局部最优解；收敛速度较快

3\. Mini-batch：中和的方法

4\. 启发式的优化算法：遗传算法，粒子群算法，设定一个目标函数。每次迭代根据相应的策略优化种群

梯度消失和梯度膨胀？

梯度消失：根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，经过多层传播后，误差对输入层的偏导会趋于0；可以采用ReLU激活函数解决

梯度膨胀：根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，经过多层传播后，误差对输入层的偏导会趋于无穷大